{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DkKzkIHjiSaY"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Gradient Descent"
      ],
      "metadata": {
        "id": "Y43ojQfLiX6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X, y, lr = 0.01, n_iters = 1000, tol = 1e-6):\n",
        "  '''\n",
        "  Args:\n",
        "    X: shape (n_samples, n_features)\n",
        "    y: shape (n_samples,)\n",
        "    lr: learning rate\n",
        "    n_iters: max iterations\n",
        "    tol: tolerance for convergence\n",
        "\n",
        "  Returns:\n",
        "    w: learned weights, shape (n_features,)\n",
        "    history: list of loss values per iteration\n",
        "\n",
        "  '''\n",
        "  n_samples, n_features = X.shape\n",
        "  w = np.zeros(n_features)\n",
        "  history = []\n",
        "\n",
        "  for i in range(n_iters):\n",
        "    preds = X.dot(w)\n",
        "    error = preds - y\n",
        "    grad = (2/n_samples) * X.T.dot(error)\n",
        "    w -= lr * grad\n",
        "    loss = np.mean(error**2)\n",
        "    history.append(loss)\n",
        "    if np.linalg.norm(grad) < tol:\n",
        "      break\n",
        "  return w, history"
      ],
      "metadata": {
        "id": "LnuMTpEqiWWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "bEKIvFlml6CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, lr = 0.01, n_epochs = 50, batch_size = 1):\n",
        "  n_samples, n_features = X.shape\n",
        "  w = np.zeros(n_features)\n",
        "  history = []\n",
        "  ''' batch size = 1 corresponds to pure SGD '''\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    perm = np.random.permutation(n_samples)\n",
        "    ''' generates a random permutation of the indices of the samples '''\n",
        "    X_shuffled = X[perm]\n",
        "    y_shuffled = y[perm]\n",
        "\n",
        "    for i in range(0, n_samples, batch_size):\n",
        "      ''' gonna extract mini-batches of features and target values '''\n",
        "      X_batch = X_shuffled[i:i+batch_size]\n",
        "      y_batch = y_shuffled[i:i+batch_size]\n",
        "      preds = X_batch.dot(w)\n",
        "      error = preds - y_batch\n",
        "      grad = (2/batch_size) * X_batch.T.dot(error)\n",
        "      w -= lr * grad\n",
        "\n",
        "    preds_full = X.dot(w)\n",
        "    loss = np.mean((preds_full - y)**2)\n",
        "    history.append(loss)\n",
        "  return w, history\n"
      ],
      "metadata": {
        "id": "J86ZaflXlKEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum Gradient Descent"
      ],
      "metadata": {
        "id": "5crQzFZtol0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def momentum_gradient_descent(X, y, lr = 0.01, n_iters = 1000, momentum = 0.9):\n",
        "  n_samples, n_features = X.shape\n",
        "  w = np.zeros(n_features)\n",
        "  v = np.zeros_like(w)\n",
        "  history = []\n",
        "\n",
        "  for i in range(n_iters):\n",
        "    preds = X.dot(w)\n",
        "    error = preds - y\n",
        "    grad = (2/n_samples) * X.T.dot(error)\n",
        "    v = momentum * v + lr * grad\n",
        "    w -= v\n",
        "    history.append(np.mean(error**2))\n",
        "  return w, history"
      ],
      "metadata": {
        "id": "mI7-khgLoi1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nesterov_gradient_descent(X, y, lr = 0.01, n_iters = 1000, momentum = 0.9):\n",
        "  n_samples, n_features = X.shape\n",
        "  w = np.zeros(n_features)\n",
        "  v = np.zeros_like(w)\n",
        "  history = []\n",
        "\n",
        "  for i in range(n_iters):\n",
        "    w_lookahead = w - momentum * v\n",
        "    preds = X.dot(w_lookahead)\n",
        "    error = preds - y\n",
        "    grad = (2/n_samples) * X.T.dot(error)\n",
        "    v = momentum * v + lr * grad\n",
        "    w -= v\n",
        "    history.append(np.mean(error**2))\n",
        "  return w, history\n"
      ],
      "metadata": {
        "id": "unM47h1Ipv_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaGrad - Adaptive Learning Rate"
      ],
      "metadata": {
        "id": "cte6-V1JlIqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adagrad(X, y, lr = 0.01, n_epochs = 100, epsilon = 1e-8):\n",
        "  n_samples, n_features = X.shape\n",
        "  w = np.zeros(n_features)\n",
        "  G = np.zeros(n_features)\n",
        "  history = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    grad_sum = np.zeros_like(w)\n",
        "    for i in range(n_samples):\n",
        "      xi = X[i]\n",
        "      yi = y[i]\n",
        "      pred = xi.dot(w)\n",
        "      error = pred - yi\n",
        "      grad = 2 * xi * error\n",
        "      G += grad**2\n",
        "      adjusted_lr = lr / (np.sqrt(G) + epsilon)\n",
        "      w -= adjusted_lr * grad\n",
        "    preds_full = X.dot(w)\n",
        "    history.append(np.mean((preds_full - y)**2))\n",
        "  return w, history\n"
      ],
      "metadata": {
        "id": "qnToS5UZqahS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsprop(X, y, lr = 0.01, n_epochs = 100, decay = 0.9, epsilon = 1e-8):\n",
        "  n_samples, n_features = X.shape\n",
        "  w = np.zeros(n_features)\n",
        "  E_g2 = np.zeros(n_features)\n",
        "  history = []\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    for i in range(n_samples):\n",
        "      xi = X[i]\n",
        "      yi = y[i]\n",
        "      pred = xi.dot(w)\n",
        "      error = pred - yi\n",
        "      grad = 2 * xi * error\n",
        "      E_g2 = decay * E_g2 + (1 - decay) * grad**2\n",
        "      adjusted_lr = lr / (np.sqrt(E_g2) + epsilon)\n",
        "      w -= adjusted_lr * grad\n",
        "    preds_full = X.dot(w)\n",
        "    history.append(np.mean((preds_full - y)**2))\n",
        "  return w, history"
      ],
      "metadata": {
        "id": "vCyYkG0WlO4y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adam(X, y, lr = 0.01, n_iters = 1000, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8):\n",
        "  n_samples, n_features = X.shape\n",
        "  w = np.zeros(n_features)\n",
        "  v = np.zeros(n_features)\n",
        "  m = np.zeros(n_features)\n",
        "  history = []\n",
        "\n",
        "  for t in range(1, n_iters + 1):\n",
        "    preds = X.dot(w)\n",
        "    error = preds - y\n",
        "    grad = (2/n_samples) * X.T.dot(error)\n",
        "\n",
        "    m = beta1 * m + (1 - beta1) * grad\n",
        "    v = beta2 * v + (1 - beta2) * grad**2\n",
        "    m_hat = m / (1 - beta1**t)\n",
        "    v_hat = v / (1 - beta2**t)\n",
        "\n",
        "    w -= lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "    history.append(np.mean(error**2))\n",
        "  return w, history"
      ],
      "metadata": {
        "id": "0lwlEIe-mRO9"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}