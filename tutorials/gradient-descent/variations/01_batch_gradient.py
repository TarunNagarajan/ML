# -*- coding: utf-8 -*-
"""01_batch_gradient.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5DvVqWxRiXCHL4o8fUi1QAjSOISPi8u
"""

import numpy as np

def batch_gradient_descent(X, y, lr = 0.01, n_iters = 1000, tol = 1e-6):
  n_samples, n_features = X.shape
  w = np.zeros(n_features)
  history = []

  for _ in range(n_iters):
    pred = np.dot(X, w)
    error = pred - y
    grad = (2/n_samples) * X.T.dot(error)

    w -= lr * grad
    loss = np.mean(error**2)
    history.append(loss)

    if np.linalg.norm(grad) < tol:
      break

  return w, history