> data(mtcars)
> head(mtcars)
                   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1

> # a simple linear model, where mpg is reliant on wt alone. 
> model_simple <- lm(mpg ~ wt, data = mtcars)
> summary(model_simple)

Call:
lm(formula = mpg ~ wt, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.5432 -2.3647 -0.1252  1.4096  6.8727 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  37.2851     1.8776  19.858  < 2e-16 ***
wt           -5.3445     0.5591  -9.559 1.29e-10 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.046 on 30 degrees of freedom
Multiple R-squared:  0.7528,	Adjusted R-squared:  0.7446 
F-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10

> # PLOT I: Simple Linear Model (mpg ~ wt)
> plot(mtcars$wt, mtcars$mpg,
+      main = "MPG vs. Wt",
+      xlab = "Weight (1000 lbs)",
+      ylab = "Miles Per Gallon",
+      pch = 19,
+      col = "blue")
> abline(model_simple, col = "red", lwd = 2)
> # multiple linear regression model.
> model_multiple <- lm(mpg ~ wt + hp, data = mtcars)
> summary(model_multiple)

Call:
lm(formula = mpg ~ wt + hp, data = mtcars)

Residuals:
   Min     1Q Median     3Q    Max 
-3.941 -1.600 -0.182  1.050  5.854 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 37.22727    1.59879  23.285  < 2e-16 ***
wt          -3.87783    0.63273  -6.129 1.12e-06 ***
hp          -0.03177    0.00903  -3.519  0.00145 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.593 on 29 degrees of freedom
Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8148 
F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12

> # using new data for a data frame, so that the model can try and make predictions. 
> wt <- 3
> hp <- 110
> new_data <- data.frame(wt = 3, hp = 110)
> predicted <- predict(model_multiple, newdata = new_data)
> cat("Predicted MPG for wt =", wt, " and hp =", hp, "is:", predicted)
Predicted MPG for wt = 3  and hp = 110 is: 22.09875
> par(mfrow = c(2, 2))
> plot(model_multiple)
> par(mfrow = c(1, 1)) # clear the plot space
> # prediction vs. actual -> a comparison
> prediction <- predict(model_multiple, newdata = mtcars) # predicted mpg
> actual <- mtcars$mpg # actual mpg
> cat("Actual Length:", length(actual))
Actual Length: 32
> cat("Predicted Length: ", length(prediction))
Predicted Length:  32
> plot(actual, prediction,
+      main = "Predicted vs. Actual MPG",
+      xlab = "Actual MPG",
+      ylab = "Predicted MPG",
+      pch = 19,
+      col = "darkgreen")
> abline(a = 0, b = 1, col = "red", lwd = 2)
> dev.off()

Iteration:  50 | Loss: 99.85701 
Iteration:  100 | Loss: 86.99216 
Iteration:  150 | Loss: 75.94285 
Iteration:  200 | Loss: 66.45287 
Iteration:  250 | Loss: 58.30217 
Iteration:  300 | Loss: 51.30173 
Iteration:  350 | Loss: 45.28922 
Iteration:  400 | Loss: 40.12523 
Iteration:  450 | Loss: 35.69001 
Iteration:  500 | Loss: 31.88071 
Iteration:  550 | Loss: 28.60899 
Iteration:  600 | Loss: 25.79899 
Iteration:  650 | Loss: 23.38556 
Iteration:  700 | Loss: 21.31272 
Iteration:  750 | Loss: 19.53241 
Iteration:  800 | Loss: 18.00334 
Iteration:  850 | Loss: 16.69006 
Iteration:  900 | Loss: 15.56212 
Iteration:  950 | Loss: 14.59336 
Iteration:  1000 | Loss: 13.76132 
Iteration:  1050 | Loss: 13.0467 
Iteration:  1100 | Loss: 12.43293 
Iteration:  1150 | Loss: 11.90577 
Iteration:  1200 | Loss: 11.45302 
Iteration:  1250 | Loss: 11.06415 
Iteration:  1300 | Loss: 10.73017 
Iteration:  1350 | Loss: 10.44332 
Iteration:  1400 | Loss: 10.19695 
Iteration:  1450 | Loss: 9.985347 
Iteration:  1500 | Loss: 9.803608 
Iteration:  1550 | Loss: 9.647517 
Iteration:  1600 | Loss: 9.513455 
Iteration:  1650 | Loss: 9.398312 
Iteration:  1700 | Loss: 9.299419 
Iteration:  1750 | Loss: 9.214482 
Iteration:  1800 | Loss: 9.141531 
Iteration:  1850 | Loss: 9.078876 
Iteration:  1900 | Loss: 9.025063 
Iteration:  1950 | Loss: 8.978844 
Iteration:  2000 | Loss: 8.939148 
Iteration:  2050 | Loss: 8.905054 
Iteration:  2100 | Loss: 8.875772 
Iteration:  2150 | Loss: 8.850622 
Iteration:  2200 | Loss: 8.829021 
Iteration:  2250 | Loss: 8.810469 
Iteration:  2300 | Loss: 8.794534 
Iteration:  2350 | Loss: 8.780849 
Iteration:  2400 | Loss: 8.769095 
Iteration:  2450 | Loss: 8.759 
Iteration:  2500 | Loss: 8.750329 
Iteration:  2550 | Loss: 8.742882 
Iteration:  2600 | Loss: 8.736486 
Iteration:  2650 | Loss: 8.730993 
Iteration:  2700 | Loss: 8.726275 
Iteration:  2750 | Loss: 8.722222 
Iteration:  2800 | Loss: 8.718742 
Iteration:  2850 | Loss: 8.715753 
Iteration:  2900 | Loss: 8.713185 
Iteration:  2950 | Loss: 8.71098 
Iteration:  3000 | Loss: 8.709086 
Iteration:  3050 | Loss: 8.70746 
Iteration:  3100 | Loss: 8.706063 
Iteration:  3150 | Loss: 8.704863 
Iteration:  3200 | Loss: 8.703832 
Iteration:  3250 | Loss: 8.702947 
Iteration:  3300 | Loss: 8.702187 
Iteration:  3350 | Loss: 8.701534 
Iteration:  3400 | Loss: 8.700973 
Iteration:  3450 | Loss: 8.700492 
Iteration:  3500 | Loss: 8.700078 
Iteration:  3550 | Loss: 8.699723 
Iteration:  3600 | Loss: 8.699418 
Iteration:  3650 | Loss: 8.699156 
Iteration:  3700 | Loss: 8.69893 
Iteration:  3750 | Loss: 8.698737 
Iteration:  3800 | Loss: 8.698571 
Iteration:  3850 | Loss: 8.698428 
Iteration:  3900 | Loss: 8.698306 
Iteration:  3950 | Loss: 8.698201 
Iteration:  4000 | Loss: 8.69811 
Iteration:  4050 | Loss: 8.698033 
Iteration:  4100 | Loss: 8.697966 
Iteration:  4150 | Loss: 8.697909 
Iteration:  4200 | Loss: 8.69786 
Iteration:  4250 | Loss: 8.697818 
Iteration:  4300 | Loss: 8.697781 
Iteration:  4350 | Loss: 8.69775 
Iteration:  4400 | Loss: 8.697723 
Iteration:  4450 | Loss: 8.6977 
Iteration:  4500 | Loss: 8.697681 
Iteration:  4550 | Loss: 8.697664 
Iteration:  4600 | Loss: 8.697649 
Iteration:  4650 | Loss: 8.697637 
Iteration:  4700 | Loss: 8.697626 
Iteration:  4750 | Loss: 8.697617 
Iteration:  4800 | Loss: 8.697609 
Iteration:  4850 | Loss: 8.697602 
Iteration:  4900 | Loss: 8.697596 
Iteration:  4950 | Loss: 8.697591 
Iteration:  5000 | Loss: 8.697587 
Iteration:  5050 | Loss: 8.697583 
Iteration:  5100 | Loss: 8.69758 
Iteration:  5150 | Loss: 8.697577 
Iteration:  5200 | Loss: 8.697575 
Iteration:  5250 | Loss: 8.697573 
Iteration:  5300 | Loss: 8.697571 
Iteration:  5350 | Loss: 8.69757 
Iteration:  5400 | Loss: 8.697568 
Iteration:  5450 | Loss: 8.697567 
Iteration:  5500 | Loss: 8.697566 
Iteration:  5550 | Loss: 8.697565 
Iteration:  5600 | Loss: 8.697565 
Iteration:  5650 | Loss: 8.697564 
Iteration:  5700 | Loss: 8.697564 
Iteration:  5750 | Loss: 8.697563 
Iteration:  5800 | Loss: 8.697563 
Iteration:  5850 | Loss: 8.697563 
Iteration:  5900 | Loss: 8.697562 
Iteration:  5950 | Loss: 8.697562 
Iteration:  6000 | Loss: 8.697562 
Iteration:  6050 | Loss: 8.697562 
Iteration:  6100 | Loss: 8.697561 
Iteration:  6150 | Loss: 8.697561 
Iteration:  6200 | Loss: 8.697561 
Iteration:  6250 | Loss: 8.697561 
Iteration:  6300 | Loss: 8.697561 
Iteration:  6350 | Loss: 8.697561 
Iteration:  6400 | Loss: 8.697561 
Iteration:  6450 | Loss: 8.697561 
Iteration:  6500 | Loss: 8.697561 
Iteration:  6550 | Loss: 8.697561 
Iteration:  6600 | Loss: 8.697561 
Iteration:  6650 | Loss: 8.697561 
Iteration:  6700 | Loss: 8.697561 
Iteration:  6750 | Loss: 8.697561 
Iteration:  6800 | Loss: 8.697561 
Iteration:  6850 | Loss: 8.697561 
Iteration:  6900 | Loss: 8.697561 
Iteration:  6950 | Loss: 8.697561 
Iteration:  7000 | Loss: 8.697561 
Iteration:  7050 | Loss: 8.697561 
Iteration:  7100 | Loss: 8.697561 
Iteration:  7150 | Loss: 8.697561 
Iteration:  7200 | Loss: 8.697561 
Iteration:  7250 | Loss: 8.697561 
Iteration:  7300 | Loss: 8.697561 
Iteration:  7350 | Loss: 8.697561 
Iteration:  7400 | Loss: 8.697561 
Iteration:  7450 | Loss: 8.697561 
Iteration:  7500 | Loss: 8.697561 
Iteration:  7550 | Loss: 8.697561 
Iteration:  7600 | Loss: 8.697561 
Iteration:  7650 | Loss: 8.697561 
Iteration:  7700 | Loss: 8.697561 
Iteration:  7750 | Loss: 8.697561 
Iteration:  7800 | Loss: 8.697561 
Iteration:  7850 | Loss: 8.697561 
Iteration:  7900 | Loss: 8.697561 
Iteration:  7950 | Loss: 8.697561 
Iteration:  8000 | Loss: 8.697561 
Iteration:  8050 | Loss: 8.697561 
Iteration:  8100 | Loss: 8.697561 
Iteration:  8150 | Loss: 8.697561 
Iteration:  8200 | Loss: 8.697561 
Iteration:  8250 | Loss: 8.697561 
Iteration:  8300 | Loss: 8.697561 
Iteration:  8350 | Loss: 8.697561 
Iteration:  8400 | Loss: 8.697561 
Iteration:  8450 | Loss: 8.697561 
Iteration:  8500 | Loss: 8.697561 
Iteration:  8550 | Loss: 8.697561 
Iteration:  8600 | Loss: 8.697561 
Iteration:  8650 | Loss: 8.697561 
Iteration:  8700 | Loss: 8.697561 
Iteration:  8750 | Loss: 8.697561 
Iteration:  8800 | Loss: 8.697561 
Iteration:  8850 | Loss: 8.697561 
Iteration:  8900 | Loss: 8.697561 
Iteration:  8950 | Loss: 8.697561 
Iteration:  9000 | Loss: 8.697561 
Iteration:  9050 | Loss: 8.697561 
Iteration:  9100 | Loss: 8.697561 
Iteration:  9150 | Loss: 8.697561 
Iteration:  9200 | Loss: 8.697561 
Iteration:  9250 | Loss: 8.697561 
Iteration:  9300 | Loss: 8.697561 
Iteration:  9350 | Loss: 8.697561 
Iteration:  9400 | Loss: 8.697561 
Iteration:  9450 | Loss: 8.697561 
Iteration:  9500 | Loss: 8.697561 
Iteration:  9550 | Loss: 8.697561 
Iteration:  9600 | Loss: 8.697561 
Iteration:  9650 | Loss: 8.697561 
Iteration:  9700 | Loss: 8.697561 
Iteration:  9750 | Loss: 8.697561 
Iteration:  9800 | Loss: 8.697561 
Iteration:  9850 | Loss: 8.697561 
Iteration:  9900 | Loss: 8.697561 
Iteration:  9950 | Loss: 8.697561 
Iteration:  10000 | Loss: 8.697561 
R-squared: 0.7528328
